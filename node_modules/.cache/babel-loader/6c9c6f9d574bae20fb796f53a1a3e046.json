{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\nexport class RMSPropOptimizer extends Optimizer {\n  constructor(learningRate) {\n    let decay = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0.9;\n    let momentum = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0.0;\n    let epsilon = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    let centered = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : false;\n    super();\n    this.learningRate = learningRate;\n    this.decay = decay;\n    this.momentum = momentum;\n    this.epsilon = epsilon;\n    this.accumulatedMeanSquares = [];\n    this.accumulatedMoments = [];\n    this.accumulatedMeanGrads = [];\n    this.centered = centered;\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n    if (learningRate == null) {\n      throw new Error(`learningRate for RMSPropOptimizer must be defined.`);\n    }\n  }\n  applyGradients(variableGradients) {\n    const variableNames = Array.isArray(variableGradients) ? variableGradients.map(item => item.name) : Object.keys(variableGradients);\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      const trainable = false;\n      if (this.accumulatedMeanSquares[i] == null) {\n        this.accumulatedMeanSquares[i] = {\n          originalName: `${name}/rms`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedMoments[i] == null) {\n        this.accumulatedMoments[i] = {\n          originalName: `${name}/momentum`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedMeanGrads[i] == null && this.centered) {\n        this.accumulatedMeanGrads[i] = {\n          originalName: `${name}/mg`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n      const accumulatedMeanSquare = this.accumulatedMeanSquares[i].variable;\n      const accumulatedMoments = this.accumulatedMoments[i].variable;\n      tidy(() => {\n        const newAccumulatedMeanSquare = add(mul(accumulatedMeanSquare, this.decay), mul(square(gradient), 1 - this.decay));\n        if (this.centered) {\n          const accumulatedMeanGrad = this.accumulatedMeanGrads[i].variable;\n          // Centered gradient\n          const newAccumulatedMeanGrad = add(mul(accumulatedMeanGrad, this.decay), mul(gradient, 1 - this.decay));\n          const gradContribution = div(mul(gradient, this.learningRate), sqrt(sub(newAccumulatedMeanSquare, add(square(newAccumulatedMeanGrad), this.epsilon))));\n          const newAccumulatedMoments = add(mul(accumulatedMoments, this.momentum), gradContribution);\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMeanGrad.assign(newAccumulatedMeanGrad);\n          accumulatedMoments.assign(newAccumulatedMoments);\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        } else {\n          // Plain gradient\n          const newAccumulatedMeanSquare = add(mul(accumulatedMeanSquare, this.decay), mul(square(gradient), 1 - this.decay));\n          const newAccumulatedMoments = add(mul(accumulatedMoments, this.momentum), div(mul(gradient, this.learningRate), sqrt(add(newAccumulatedMeanSquare, this.epsilon))));\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMoments.assign(newAccumulatedMoments);\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        }\n      });\n    });\n    this.incrementIterations();\n  }\n  dispose() {\n    if (this.accumulatedMeanSquares != null) {\n      dispose(this.accumulatedMeanSquares.map(v => v.variable));\n    }\n    if (this.accumulatedMeanGrads != null && this.centered) {\n      dispose(this.accumulatedMeanGrads.map(v => v.variable));\n    }\n    if (this.accumulatedMoments != null) {\n      dispose(this.accumulatedMoments.map(v => v.variable));\n    }\n  }\n  async getWeights() {\n    // Order matters for Python compatibility.\n    const variables = [...this.accumulatedMeanSquares, ...this.accumulatedMoments];\n    if (this.centered) {\n      variables.push(...this.accumulatedMeanGrads);\n    }\n    return [await this.saveIterations()].concat(variables.map(v => ({\n      name: v.originalName,\n      tensor: v.variable\n    })));\n  }\n  async setWeights(weightValues) {\n    weightValues = await this.extractIterations(weightValues);\n    const variableCount = this.centered ? weightValues.length / 3 : weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedMeanSquares = weightValues.slice(0, variableCount).map(v => ({\n      originalName: v.name,\n      variable: v.tensor.variable(trainable)\n    }));\n    this.accumulatedMoments = weightValues.slice(variableCount, variableCount * 2).map(v => ({\n      originalName: v.name,\n      variable: v.tensor.variable(trainable)\n    }));\n    if (this.centered) {\n      this.accumulatedMeanGrads = weightValues.slice(variableCount * 2, variableCount * 3).map(v => ({\n        originalName: v.name,\n        variable: v.tensor.variable(trainable)\n      }));\n    }\n  }\n  getConfig() {\n    return {\n      'learningRate': this.learningRate,\n      'decay': this.decay,\n      'momentum': this.momentum,\n      'epsilon': this.epsilon,\n      'centered': this.centered\n    };\n  }\n  /** @nocollapse */\n  static fromConfig(cls, config) {\n    return new cls(config['learningRate'], config['decay'], config['momentum'], config['epsilon'], config['centered']);\n  }\n}\n/** @nocollapse */\nRMSPropOptimizer.className = 'RMSProp'; // Note: Name matters for Python compatibility.\nregisterClass(RMSPropOptimizer);","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAM,QAAO,WAAW;AAChC,SAAQC,OAAO,EAAEC,IAAI,QAAO,YAAY;AACxC,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,IAAI,QAAO,aAAa;AAChC,SAAQC,MAAM,QAAO,eAAe;AACpC,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,SAAS,QAAO,mBAAmB;AAC3C,SAAoBC,aAAa,QAA8C,kBAAkB;AAGjG,SAAQC,SAAS,QAA0B,aAAa;AAExD;AACA,OAAM,MAAOC,gBAAiB,SAAQD,SAAS;EAS7CE,YACcC,YAAoB,EAEd;IAAA,IAF0BC,4EAAQ,GAAG;IAAA,IAC3CC,+EAAW,GAAG;IAAA,IAAYC,8EAAkB,IAAI;IAAA,IAC1DC,QAAQ,uEAAG,KAAK;IAClB,KAAK,EAAE;IAHK,iBAAY,GAAZJ,YAAY;IAAoB,UAAK,GAALC,KAAK;IACrC,aAAQ,GAARC,QAAQ;IAAkB,YAAO,GAAPC,OAAO;IANvC,2BAAsB,GAAwB,EAAE;IAChD,uBAAkB,GAAwB,EAAE;IAC5C,yBAAoB,GAAwB,EAAE;IAQpD,IAAI,CAACC,QAAQ,GAAGA,QAAQ;IAExB,IAAID,OAAO,IAAI,IAAI,EAAE;MACnB,IAAI,CAACA,OAAO,GAAGjB,MAAM,CAACmB,OAAO,CAACF,OAAO,EAAE;;IAEzC,IAAIH,YAAY,IAAI,IAAI,EAAE;MACxB,MAAM,IAAIM,KAAK,CAAC,oDAAoD,CAAC;;EAEzE;EAEAC,cAAc,CAACC,iBAA+C;IAC5D,MAAMC,aAAa,GAAGC,KAAK,CAACC,OAAO,CAACH,iBAAiB,CAAC,GAClDA,iBAAiB,CAACI,GAAG,CAACC,IAAI,IAAIA,IAAI,CAACC,IAAI,CAAC,GACxCC,MAAM,CAACC,IAAI,CAACR,iBAAiB,CAAC;IAElCC,aAAa,CAACQ,OAAO,CAAC,CAACH,IAAI,EAAEI,CAAC,KAAI;MAChC,MAAMC,KAAK,GAAGjC,MAAM,CAACkC,mBAAmB,CAACN,IAAI,CAAC;MAC9C,MAAMO,SAAS,GAAG,KAAK;MACvB,IAAI,IAAI,CAACC,sBAAsB,CAACJ,CAAC,CAAC,IAAI,IAAI,EAAE;QAC1C,IAAI,CAACI,sBAAsB,CAACJ,CAAC,CAAC,GAAG;UAC/BK,YAAY,EAAE,GAAGT,IAAI,MAAM;UAC3BU,QAAQ,EAAEpC,IAAI,CAAC,MAAMO,SAAS,CAACwB,KAAK,CAAC,CAACK,QAAQ,CAACH,SAAS,CAAC;SAC1D;;MAEH,IAAI,IAAI,CAACI,kBAAkB,CAACP,CAAC,CAAC,IAAI,IAAI,EAAE;QACtC,IAAI,CAACO,kBAAkB,CAACP,CAAC,CAAC,GAAG;UAC3BK,YAAY,EAAE,GAAGT,IAAI,WAAW;UAChCU,QAAQ,EAAEpC,IAAI,CAAC,MAAMO,SAAS,CAACwB,KAAK,CAAC,CAACK,QAAQ,CAACH,SAAS,CAAC;SAC1D;;MAEH,IAAI,IAAI,CAACK,oBAAoB,CAACR,CAAC,CAAC,IAAI,IAAI,IAAI,IAAI,CAACd,QAAQ,EAAE;QACzD,IAAI,CAACsB,oBAAoB,CAACR,CAAC,CAAC,GAAG;UAC7BK,YAAY,EAAE,GAAGT,IAAI,KAAK;UAC1BU,QAAQ,EAAEpC,IAAI,CAAC,MAAMO,SAAS,CAACwB,KAAK,CAAC,CAACK,QAAQ,CAACH,SAAS,CAAC;SAC1D;;MAGH,MAAMM,QAAQ,GAAGjB,KAAK,CAACC,OAAO,CAACH,iBAAiB,CAAC,GAC7CA,iBAAiB,CAACU,CAAC,CAAC,CAACU,MAAM,GAC3BpB,iBAAiB,CAACM,IAAI,CAAC;MAC3B,IAAIa,QAAQ,IAAI,IAAI,EAAE;QACpB;;MAGF,MAAME,qBAAqB,GAAG,IAAI,CAACP,sBAAsB,CAACJ,CAAC,CAAC,CAACM,QAAQ;MACrE,MAAMC,kBAAkB,GAAG,IAAI,CAACA,kBAAkB,CAACP,CAAC,CAAC,CAACM,QAAQ;MAC9DpC,IAAI,CAAC,MAAK;QACR,MAAM0C,wBAAwB,GAC1BzC,GAAG,CAACE,GAAG,CAACsC,qBAAqB,EAAE,IAAI,CAAC5B,KAAK,CAAC,EACtCV,GAAG,CAACE,MAAM,CAACkC,QAAQ,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC1B,KAAK,CAAC,CAAC;QAE9C,IAAI,IAAI,CAACG,QAAQ,EAAE;UACjB,MAAM2B,mBAAmB,GAAG,IAAI,CAACL,oBAAoB,CAACR,CAAC,CAAC,CAACM,QAAQ;UACjE;UACA,MAAMQ,sBAAsB,GACxB3C,GAAG,CAACE,GAAG,CAACwC,mBAAmB,EAAE,IAAI,CAAC9B,KAAK,CAAC,EACpCV,GAAG,CAACoC,QAAQ,EAAE,CAAC,GAAG,IAAI,CAAC1B,KAAK,CAAC,CAAC;UAEtC,MAAMgC,gBAAgB,GAClB3C,GAAG,CAACC,GAAG,CAACoC,QAAQ,EAAE,IAAI,CAAC3B,YAAY,CAAC,EAChCR,IAAI,CACAE,GAAG,CAACoC,wBAAwB,EACxBzC,GAAG,CAACI,MAAM,CAACuC,sBAAsB,CAAC,EAAE,IAAI,CAAC7B,OAAO,CAAC,CAAC,CAAC,CAAC;UACpE,MAAM+B,qBAAqB,GACvB7C,GAAG,CAACE,GAAG,CAACkC,kBAAkB,EAAE,IAAI,CAACvB,QAAQ,CAAC,EAAE+B,gBAAgB,CAAC;UAEjEJ,qBAAqB,CAACM,MAAM,CAACL,wBAAwB,CAAC;UACtDC,mBAAmB,CAACI,MAAM,CAACH,sBAAsB,CAAC;UAClDP,kBAAkB,CAACU,MAAM,CAACD,qBAAqB,CAAC;UAEhD,MAAME,QAAQ,GAAG1C,GAAG,CAACyB,KAAK,EAAEe,qBAAqB,CAAC;UAClDf,KAAK,CAACgB,MAAM,CAACC,QAAQ,CAAC;SACvB,MAAM;UACL;UACA,MAAMN,wBAAwB,GAC1BzC,GAAG,CAACE,GAAG,CAACsC,qBAAqB,EAAE,IAAI,CAAC5B,KAAK,CAAC,EACtCV,GAAG,CAACE,MAAM,CAACkC,QAAQ,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC1B,KAAK,CAAC,CAAC;UAE9C,MAAMiC,qBAAqB,GACvB7C,GAAG,CAACE,GAAG,CAACkC,kBAAkB,EAAE,IAAI,CAACvB,QAAQ,CAAC,EACtCZ,GAAG,CAACC,GAAG,CAACoC,QAAQ,EAAE,IAAI,CAAC3B,YAAY,CAAC,EAChCR,IAAI,CAACH,GAAG,CAACyC,wBAAwB,EAAE,IAAI,CAAC3B,OAAO,CAAC,CAAC,CAAC,CAAC;UAE/D0B,qBAAqB,CAACM,MAAM,CAACL,wBAAwB,CAAC;UACtDL,kBAAkB,CAACU,MAAM,CAACD,qBAAqB,CAAC;UAEhD,MAAME,QAAQ,GAAG1C,GAAG,CAACyB,KAAK,EAAEe,qBAAqB,CAAC;UAClDf,KAAK,CAACgB,MAAM,CAACC,QAAQ,CAAC;;MAE1B,CAAC,CAAC;IACJ,CAAC,CAAC;IACF,IAAI,CAACC,mBAAmB,EAAE;EAC5B;EAEAlD,OAAO;IACL,IAAI,IAAI,CAACmC,sBAAsB,IAAI,IAAI,EAAE;MACvCnC,OAAO,CAAC,IAAI,CAACmC,sBAAsB,CAACV,GAAG,CAAC0B,CAAC,IAAIA,CAAC,CAACd,QAAQ,CAAC,CAAC;;IAE3D,IAAI,IAAI,CAACE,oBAAoB,IAAI,IAAI,IAAI,IAAI,CAACtB,QAAQ,EAAE;MACtDjB,OAAO,CAAC,IAAI,CAACuC,oBAAoB,CAACd,GAAG,CAAC0B,CAAC,IAAIA,CAAC,CAACd,QAAQ,CAAC,CAAC;;IAEzD,IAAI,IAAI,CAACC,kBAAkB,IAAI,IAAI,EAAE;MACnCtC,OAAO,CAAC,IAAI,CAACsC,kBAAkB,CAACb,GAAG,CAAC0B,CAAC,IAAIA,CAAC,CAACd,QAAQ,CAAC,CAAC;;EAEzD;EAEA,MAAMe,UAAU;IACd;IACA,MAAMC,SAAS,GACX,CAAC,GAAG,IAAI,CAAClB,sBAAsB,EAAE,GAAG,IAAI,CAACG,kBAAkB,CAAC;IAChE,IAAI,IAAI,CAACrB,QAAQ,EAAE;MACjBoC,SAAS,CAACC,IAAI,CAAC,GAAG,IAAI,CAACf,oBAAoB,CAAC;;IAE9C,OAAO,CAAC,MAAM,IAAI,CAACgB,cAAc,EAAE,CAAC,CAACC,MAAM,CACvCH,SAAS,CAAC5B,GAAG,CAAC0B,CAAC,KAAK;MAACxB,IAAI,EAAEwB,CAAC,CAACf,YAAY;MAAEK,MAAM,EAAEU,CAAC,CAACd;IAAQ,CAAC,CAAC,CAAC,CAAC;EACvE;EAEA,MAAMoB,UAAU,CAACC,YAA2B;IAC1CA,YAAY,GAAG,MAAM,IAAI,CAACC,iBAAiB,CAACD,YAAY,CAAC;IACzD,MAAME,aAAa,GACf,IAAI,CAAC3C,QAAQ,GAAGyC,YAAY,CAACG,MAAM,GAAG,CAAC,GAAGH,YAAY,CAACG,MAAM,GAAG,CAAC;IACrE,MAAM3B,SAAS,GAAG,KAAK;IACvB,IAAI,CAACC,sBAAsB,GACvBuB,YAAY,CAACI,KAAK,CAAC,CAAC,EAAEF,aAAa,CAAC,CAACnC,GAAG,CAAC0B,CAAC,KAAK;MACJf,YAAY,EAAEe,CAAC,CAACxB,IAAI;MACpBU,QAAQ,EAAEc,CAAC,CAACV,MAAM,CAACJ,QAAQ,CACvBH,SAAS;KACd,CAAC,CAAC;IAChD,IAAI,CAACI,kBAAkB,GACnBoB,YAAY,CAACI,KAAK,CAACF,aAAa,EAAEA,aAAa,GAAG,CAAC,CAAC,CAC/CnC,GAAG,CAAC0B,CAAC,KAAK;MACJf,YAAY,EAAEe,CAAC,CAACxB,IAAI;MACpBU,QAAQ,EAAEc,CAAC,CAACV,MAAM,CAACJ,QAAQ,CAACH,SAAS;KACtC,CAAC,CAAC;IAChB,IAAI,IAAI,CAACjB,QAAQ,EAAE;MACjB,IAAI,CAACsB,oBAAoB,GACrBmB,YAAY,CAACI,KAAK,CAACF,aAAa,GAAG,CAAC,EAAEA,aAAa,GAAG,CAAC,CAAC,CACnDnC,GAAG,CAAC0B,CAAC,KAAK;QACJf,YAAY,EAAEe,CAAC,CAACxB,IAAI;QACpBU,QAAQ,EAAEc,CAAC,CAACV,MAAM,CAACJ,QAAQ,CAACH,SAAS;OACtC,CAAC,CAAC;;EAEpB;EAEA6B,SAAS;IACP,OAAO;MACL,cAAc,EAAE,IAAI,CAAClD,YAAY;MACjC,OAAO,EAAE,IAAI,CAACC,KAAK;MACnB,UAAU,EAAE,IAAI,CAACC,QAAQ;MACzB,SAAS,EAAE,IAAI,CAACC,OAAO;MACvB,UAAU,EAAE,IAAI,CAACC;KAClB;EACH;EAEA;EACA,OAAO+C,UAAU,CACbC,GAA+B,EAAEC,MAAkB;IACrD,OAAO,IAAID,GAAG,CACVC,MAAM,CAAC,cAAc,CAAC,EAAEA,MAAM,CAAC,OAAO,CAAC,EAAEA,MAAM,CAAC,UAAU,CAAC,EAC3DA,MAAM,CAAC,SAAS,CAAC,EAAEA,MAAM,CAAC,UAAU,CAAC,CAAC;EAC5C;;AA9KA;AACOvD,0BAAS,GAAG,SAAS,CAAC,CAAE;AA+KjCF,aAAa,CAACE,gBAAgB,CAAC","names":["ENGINE","dispose","tidy","add","div","mul","sqrt","square","sub","zerosLike","registerClass","Optimizer","RMSPropOptimizer","constructor","learningRate","decay","momentum","epsilon","centered","backend","Error","applyGradients","variableGradients","variableNames","Array","isArray","map","item","name","Object","keys","forEach","i","value","registeredVariables","trainable","accumulatedMeanSquares","originalName","variable","accumulatedMoments","accumulatedMeanGrads","gradient","tensor","accumulatedMeanSquare","newAccumulatedMeanSquare","accumulatedMeanGrad","newAccumulatedMeanGrad","gradContribution","newAccumulatedMoments","assign","newValue","incrementIterations","v","getWeights","variables","push","saveIterations","concat","setWeights","weightValues","extractIterations","variableCount","length","slice","getConfig","fromConfig","cls","config"],"sources":["C:\\Users\\vivek\\OneDrive\\Desktop\\College\\z#5_Internships\\IISC\\tfjs-core\\src\\optimizers\\rmsprop_optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {dispose, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {div} from '../ops/div';\nimport {mul} from '../ops/mul';\nimport {sqrt} from '../ops/sqrt';\nimport {square} from '../ops/square';\nimport {sub} from '../ops/sub';\nimport {zerosLike} from '../ops/zeros_like';\nimport {ConfigDict, registerClass, Serializable, SerializableConstructor} from '../serialization';\nimport {NamedTensor, NamedTensorMap} from '../tensor_types';\n\nimport {Optimizer, OptimizerVariable} from './optimizer';\n\n/** @doclink Optimizer */\nexport class RMSPropOptimizer extends Optimizer {\n  /** @nocollapse */\n  static className = 'RMSProp';  // Note: Name matters for Python compatibility.\n  private centered: boolean;\n\n  private accumulatedMeanSquares: OptimizerVariable[] = [];\n  private accumulatedMoments: OptimizerVariable[] = [];\n  private accumulatedMeanGrads: OptimizerVariable[] = [];\n\n  constructor(\n      protected learningRate: number, protected decay = 0.9,\n      protected momentum = 0.0, protected epsilon: number = null,\n      centered = false) {\n    super();\n\n    this.centered = centered;\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n    if (learningRate == null) {\n      throw new Error(`learningRate for RMSPropOptimizer must be defined.`);\n    }\n  }\n\n  applyGradients(variableGradients: NamedTensorMap|NamedTensor[]) {\n    const variableNames = Array.isArray(variableGradients) ?\n        variableGradients.map(item => item.name) :\n        Object.keys(variableGradients);\n\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      const trainable = false;\n      if (this.accumulatedMeanSquares[i] == null) {\n        this.accumulatedMeanSquares[i] = {\n          originalName: `${name}/rms`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedMoments[i] == null) {\n        this.accumulatedMoments[i] = {\n          originalName: `${name}/momentum`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedMeanGrads[i] == null && this.centered) {\n        this.accumulatedMeanGrads[i] = {\n          originalName: `${name}/mg`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      const gradient = Array.isArray(variableGradients) ?\n          variableGradients[i].tensor :\n          variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n\n      const accumulatedMeanSquare = this.accumulatedMeanSquares[i].variable;\n      const accumulatedMoments = this.accumulatedMoments[i].variable;\n      tidy(() => {\n        const newAccumulatedMeanSquare =\n            add(mul(accumulatedMeanSquare, this.decay),\n                mul(square(gradient), 1 - this.decay));\n\n        if (this.centered) {\n          const accumulatedMeanGrad = this.accumulatedMeanGrads[i].variable;\n          // Centered gradient\n          const newAccumulatedMeanGrad =\n              add(mul(accumulatedMeanGrad, this.decay),\n                  mul(gradient, 1 - this.decay));\n\n          const gradContribution =\n              div(mul(gradient, this.learningRate),\n                  sqrt(\n                      sub(newAccumulatedMeanSquare,\n                          add(square(newAccumulatedMeanGrad), this.epsilon))));\n          const newAccumulatedMoments =\n              add(mul(accumulatedMoments, this.momentum), gradContribution);\n\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMeanGrad.assign(newAccumulatedMeanGrad);\n          accumulatedMoments.assign(newAccumulatedMoments);\n\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        } else {\n          // Plain gradient\n          const newAccumulatedMeanSquare =\n              add(mul(accumulatedMeanSquare, this.decay),\n                  mul(square(gradient), 1 - this.decay));\n\n          const newAccumulatedMoments =\n              add(mul(accumulatedMoments, this.momentum),\n                  div(mul(gradient, this.learningRate),\n                      sqrt(add(newAccumulatedMeanSquare, this.epsilon))));\n\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMoments.assign(newAccumulatedMoments);\n\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        }\n      });\n    });\n    this.incrementIterations();\n  }\n\n  dispose(): void {\n    if (this.accumulatedMeanSquares != null) {\n      dispose(this.accumulatedMeanSquares.map(v => v.variable));\n    }\n    if (this.accumulatedMeanGrads != null && this.centered) {\n      dispose(this.accumulatedMeanGrads.map(v => v.variable));\n    }\n    if (this.accumulatedMoments != null) {\n      dispose(this.accumulatedMoments.map(v => v.variable));\n    }\n  }\n\n  async getWeights(): Promise<NamedTensor[]> {\n    // Order matters for Python compatibility.\n    const variables: OptimizerVariable[] =\n        [...this.accumulatedMeanSquares, ...this.accumulatedMoments];\n    if (this.centered) {\n      variables.push(...this.accumulatedMeanGrads);\n    }\n    return [await this.saveIterations()].concat(\n        variables.map(v => ({name: v.originalName, tensor: v.variable})));\n  }\n\n  async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    const variableCount =\n        this.centered ? weightValues.length / 3 : weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedMeanSquares =\n        weightValues.slice(0, variableCount).map(v => ({\n                                                   originalName: v.name,\n                                                   variable: v.tensor.variable(\n                                                       trainable)\n                                                 }));\n    this.accumulatedMoments =\n        weightValues.slice(variableCount, variableCount * 2)\n            .map(v => ({\n                   originalName: v.name,\n                   variable: v.tensor.variable(trainable)\n                 }));\n    if (this.centered) {\n      this.accumulatedMeanGrads =\n          weightValues.slice(variableCount * 2, variableCount * 3)\n              .map(v => ({\n                     originalName: v.name,\n                     variable: v.tensor.variable(trainable)\n                   }));\n    }\n  }\n\n  getConfig(): ConfigDict {\n    return {\n      'learningRate': this.learningRate,\n      'decay': this.decay,\n      'momentum': this.momentum,\n      'epsilon': this.epsilon,\n      'centered': this.centered\n    };\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(\n        config['learningRate'], config['decay'], config['momentum'],\n        config['epsilon'], config['centered']);\n  }\n}\nregisterClass(RMSPropOptimizer);\n"]},"metadata":{},"sourceType":"module"}